{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m A\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from re import A\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test': 2, 'Train': 2, 'Validation': 2, 'val': 2}\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"../../.cache/kagglehub/datasets/manjilkarki/deepfake-and-real-images/versions/1/Dataset\"\n",
    "number_of_images = {}\n",
    "\n",
    "if os.path.exists(ROOT_DIR) and os.path.isdir(ROOT_DIR):\n",
    "    for dir_name in os.listdir(ROOT_DIR):\n",
    "        dir_path = os.path.join(ROOT_DIR, dir_name)\n",
    "        if os.path.isdir(dir_path):  # Ensure it's a folder\n",
    "            number_of_images[dir_name] = len(os.listdir(dir_path))\n",
    "\n",
    "    print(number_of_images)\n",
    "else:\n",
    "    print(f\"Error: '{ROOT_DIR}' is not a valid directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Real', 0), ('Fake', 24496)])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_face_path = \"./Dataset/Train/Real\"\n",
    "fake_face_path = \"./Dataset/Train/Fake\"\n",
    "\n",
    "os.makedirs(\"./Dataset/Train\", exist_ok=True)\n",
    "os.makedirs(real_face_path, exist_ok=True)\n",
    "os.makedirs(fake_face_path, exist_ok=True)\n",
    "\n",
    "for folder in os.listdir(ROOT_DIR + \"/Train\"):\n",
    "    folder_path = os.path.join(ROOT_DIR + \"/Train\", folder)\n",
    "    if os.path.isdir(folder_path): \n",
    "        files = os.listdir(folder_path)\n",
    "        target_path = real_face_path if folder == \"Real\" else fake_face_path\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_path, file) \n",
    "            if os.path.isfile(file_path):\n",
    "                shutil.move(file_path, os.path.join(target_path, file))\n",
    "\n",
    "num_files_to_select = int(0.7 * len(files))\n",
    "selected_files = np.random.choice(files, num_files_to_select, replace=False)\n",
    "\n",
    "for file in selected_files:\n",
    "      file_path = os.path.join(folder_path, file)\n",
    "      if os.path.isfile(file_path):  # Ensure it's a file\n",
    "        shutil.copy(file_path, target_path)\n",
    "\n",
    "number_of_images_train = {}\n",
    "\n",
    "for dir in os.listdir(ROOT_DIR + \"/Train\"):\n",
    "  number_of_images_train[dir] = len(os.listdir(os.path.join(\"Dataset/Train\", dir)))\n",
    "\n",
    "number_of_images_train.items()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_face_path = \"./Dataset/val/Real\"\n",
    "# fake_face_path = \"./Dataset/val/Fake\"\n",
    "\n",
    "# os.makedirs(\"./Dataset/val\", exist_ok=True)\n",
    "# os.makedirs(real_face_path, exist_ok=True)\n",
    "# os.makedirs(fake_face_path, exist_ok=True)\n",
    "\n",
    "# files = [file for file in os.listdir(\"./Dataset/Train/fake\") if os.path.isfile(os.path.join(\"./Dataset/Train/fake\", file))]\n",
    "\n",
    "# # Select 30% of the files randomly\n",
    "# num_files_to_move = int(0.3 * len(files))\n",
    "# selected_files = np.random.choice(files, num_files_to_move, replace=False)\n",
    "\n",
    "# for file in selected_files:\n",
    "#     file_path = os.path.join(\"./Dataset/Train/Fake\", file)\n",
    "#     if os.path.isfile(file_path):\n",
    "#         # Move files with 'fake' in the name to the 'Fake' folder\n",
    "#         if 'fake' in file.lower():\n",
    "#             shutil.move(file_path, os.path.join(fake_face_path, file))\n",
    "\n",
    "# files = [file for file in os.listdir(\"./Dataset/val/Real\") if os.path.isfile(os.path.join(\"./Dataset/val/Real\", file))]\n",
    "\n",
    "# Select 30% of the files randomly\n",
    "num_files_to_move = int(0.7 * len(files))\n",
    "selected_files = np.random.choice(files, num_files_to_move, replace=False)\n",
    "\n",
    "for file in os.listdir(\"./Dataset/val/Real\"):\n",
    "    file_path = os.path.join(\"./Dataset/val/Real\", file)\n",
    "    if os.path.isfile(file_path):\n",
    "        # Move files with 'fake' in the name to the 'Fake' folder\n",
    "        if 'real' in file.lower():\n",
    "            shutil.move(file_path, os.path.join(\"./Dataset/Train/Real\", file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_face_path = \"/Dataset/val/Real\"\n",
    "# fake_face_path = \"/Dataset/val/Fake\"\n",
    "\n",
    "# os.makedirs(\"./Dataset/val\", exist_ok=True)\n",
    "# os.makedirs(real_face_path, exist_ok=True)\n",
    "# os.makedirs(fake_face_path, exist_ok=True)\n",
    "\n",
    "# for folder in os.listdir(ROOT_DIR + \"/Train\"):\n",
    "#   folder_path = os.path.join(ROOT_DIR + \"/Train\", folder)\n",
    "#   if os.path.isdir(folder_path):\n",
    "#     target_path = real_face_path if folder in real_classes else fake_face_path\n",
    "#     files = os.listdir(folder_path)\n",
    "\n",
    "#     num_files_to_select = int(0.3 * len(files))\n",
    "#     selected_files = np.random.choice(files, num_files_to_select, replace=False)\n",
    "    \n",
    "#     for file in selected_files:\n",
    "#       file_path = os.path.join(folder_path, file)\n",
    "#       if os.path.isfile(file_path): \n",
    "#         shutil.copy(file_path, target_path)\n",
    "\n",
    "# TEST_DIR = ROOT_DIR + \"/content/Testing\"\n",
    "\n",
    "# real_face_path = ROOT_DIR + \"/content/test/real\"\n",
    "# fake_face_path = ROOT_DIR + \"/content/test/fake\"\n",
    "\n",
    "# os.makedirs(ROOT_DIR + \"/Dataset/test\", exist_ok=True)\n",
    "# os.makedirs(real_face_path, exist_ok=True)\n",
    "# os.makedirs(fake_face_path, exist_ok=True)\n",
    "\n",
    "# fake_classes = [\"false\"]\n",
    "# real_classes = [\"true\"]\n",
    "\n",
    "# for folder in os.listdir(TEST_DIR):\n",
    "#   folder_path = os.path.join(TEST_DIR, folder)\n",
    "#   if os.path.isdir(folder_path):\n",
    "#     target_path = real_face_path if folder in real_classes else fake_face_path\n",
    "#     for file in os.listdir(folder_path):\n",
    "#         file_path = os.path.join(folder_path, file)\n",
    "#         if os.path.isfile(file_path):\n",
    "#             shutil.copy(file_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create a test folder\n",
    "TEST_DIR = \"../../.cache/kagglehub/datasets/manjilkarki/deepfake-and-real-images/versions/1/Dataset/Test\"\n",
    "\n",
    "real_face_path = \"./Dataset/test/Real\"\n",
    "fake_face_path = \"./Dataset/test/Fake\"\n",
    "\n",
    "# Create the directories\n",
    "os.makedirs('./Dataset/test', exist_ok=True)\n",
    "os.makedirs(real_face_path, exist_ok=True)\n",
    "os.makedirs(fake_face_path, exist_ok=True)\n",
    "\n",
    "for folder in os.listdir(TEST_DIR):\n",
    "  folder_path = os.path.join(TEST_DIR, folder)\n",
    "  if os.path.isdir(folder_path):\n",
    "    target_path = real_face_path if folder == \"Real\" else fake_face_path\n",
    "# Copy all files to the appropriate folder\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if os.path.isfile(file_path):  # Ensure it's a file\n",
    "            shutil.copy(file_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization, GlobalAvgPool2D\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters= 16 , kernel_size= (3, 3), activation = 'relu', input_shape = (224,224,3) ))\n",
    "\n",
    "model.add(Conv2D(filters= 36 , kernel_size= (3, 3), activation = 'relu' ))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters= 64 , kernel_size= (3, 3), activation = 'relu' ))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(rate= 0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation= 'relu'))\n",
    "model.add(Dropout(rate= 0.25 ))\n",
    "model.add(Dense(units= 1, activation= 'sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss= keras.losses.binary_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data using Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.mobilenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingImages(path):\n",
    "  image_data = ImageDataGenerator(zoom_range=0.2, shear_range= 0.2, preprocessing_function= preprocess_input, horizontal_flip= True)\n",
    "  image = image_data.flow_from_directory(directory = path, target_size = (224, 224), batch_size = 32, class_mode = 'binary')\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Dataset/Train\"\n",
    "train_data = preprocessingImages(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingImages2(path):\n",
    "  image_data = ImageDataGenerator(preprocessing_function= preprocess_input)\n",
    "  image = image_data.flow_from_directory(directory = path, target_size = (224, 224), batch_size = 32, class_mode = 'binary')\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Dataset/test\"\n",
    "test_data = preprocessingImages2(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Dataset/val\"\n",
    "val_data = preprocessingImages2(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping and model check point\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# early stopping\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_accuracy\", min_delta= 0.01, patience= 3, verbose= 1, mode = 'auto')\n",
    "\n",
    "# model check point\n",
    "\n",
    "mc = ModelCheckpoint(monitor=\"val_accuracy\",filepath=\"./bestmodel.keras\", verbose= 1, save_best_only= True, mode = 'auto')\n",
    "\n",
    "cd = [es,mc]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
